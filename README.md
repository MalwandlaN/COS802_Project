# Red Ink & Red Flags: Correcting Benchmarks and Classifying Errors in African Machine Translation

* **Author:** Malwandla Ngobeni
* **Course:** COS802

## Overview
This project investigates the impact of benchmark data quality on Machine Translation (MT) evaluation for low-resource African languages. It re-evaluates state-of-the-art models (NLLB-200) using manually corrected FLORES datasets and introduces a novel LLM-based method for diagnosing semantic errors.

## Repository Structure
* **data**: Contains a text document with links to the aligned 'devtest' split datasets (Original vs. Corrected) sourced from the `flores-fix-4-africa` repository.
* **results**: Contains the raw experimental data generated by the project:
    * `all_hypotheses.json`: The raw translations generated by NLLB and MarianMT.
    * `final_llm_analysis.csv`: The error categorization output from the Gemini LLM analysis.
* **main_analysis.ipynb**: The complete executable Python notebook containing the experimental pipeline.
* **requirements**: A list of libraries you will need to install to run your code.

## Setup & Reproducibility

1. **Install Dependencies:**
   Ensure you have Python installed, then run:
   `pip install -r requirements.txt`

2. **API Keys:**
   To run the Phase 2 (LLM Analysis) section, you need a Google Gemini API key.
   Set your key in the notebook variable: `GENAI_API_KEY`.

3. **Hardware:**
   The translation phase (Phase 1) requires a GPU (e.g., Google Colab T4). The analysis phase (Phase 2) can run on a standard CPU.

## Key Experiments
1. **Baseline Re-evaluation:** Measuring BLEU/BERTScore shifts between original and corrected data.
2. **LLM Error Clustering:** Using Gemini-2.5-Flash to classify errors into Semantic, Lexical, and Grammatical categories.

## Acknowledgements
This work utilises the FLORES corrected datasets provided by Abdulmumin et al. (2024) and models from Meta AI (NLLB) and Helsinki-NLP.
